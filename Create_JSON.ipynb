{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alinehbg/CRP/blob/Anmol_Trial/Extract%20Names/Extract_Wiki_Data_names_try_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGVdC2zoVCeE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORsGLFfNJLAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba8e221-3831-4191-cff6-a12923bb10ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5qYvvhyqjK9"
      },
      "outputs": [],
      "source": [
        "names = pd.read_excel('/content/drive/MyDrive/CRP/Datasets/Dataset.xlsx')\n",
        "names['Name'] = names.apply (lambda x: x['Name'].replace(\".\",\"\") , axis = 1)\n",
        "names['Name_Length'] = names.apply(lambda x: len(x['Name'].split(\" \")), axis = 1)\n",
        "names['First_Name_Length'] = names.apply(lambda x: len(x['Name'].split(\" \")[0]), axis = 1)\n",
        "names = names.loc[names['Name_Length'] == 2]\n",
        "names = names.loc[names['First_Name_Length'] > 1]\n",
        "names['Name'] = names.apply(lambda x: x['Name'].title(), axis = 1 )\n",
        "names = names[[\"Name\"]]\n",
        "names_list = list(names['Name'])\n",
        "names_list = list(set(names_list))\n",
        "names.to_csv('/content/drive/MyDrive/CRP/Datasets/Names_list.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d5mnOB_N2wt"
      },
      "outputs": [],
      "source": [
        "names_chunk = pd.read_csv('/content/drive/MyDrive/CRP/Datasets/Names_list.csv') #Change the location to you local file with your name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the location to your location folder\n",
        "exist_fname_path = '/content/drive/MyDrive/CRP/Datasets/JSON/first_names.txt'\n",
        "exist_lname_path = '/content/drive/MyDrive/CRP/Datasets/JSON/last_names.txt'\n",
        "similar_fname_path = '/content/drive/MyDrive/CRP/Datasets/JSON/first_names.json'\n",
        "similar_lname_path = '/content/drive/MyDrive/CRP/Datasets/JSON/last_names.json'"
      ],
      "metadata": {
        "id": "wgHgPNTcZ36v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67upHfNtN9mQ"
      },
      "source": [
        "# functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_wJ0gTQVwW0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QROZ9l8GKICt"
      },
      "outputs": [],
      "source": [
        "def get_itemlabel(query): \n",
        "  name_list = []\n",
        "  for x in query[\"results\"][\"bindings\"]: \n",
        "    name_list.append(x[\"itemLabel\"][\"value\"])\n",
        "  return name_list\n",
        "\n",
        "def retry(data): \n",
        "  response = requests.post('https://query.wikidata.org/sparql?format=json', data=data)\n",
        "  while response.status_code == 429:\n",
        "    retry_after = response.headers.get(\"Retry-After\")\n",
        "    if retry_after:\n",
        "      # If the value of \"Retry-After\" is a number, wait that many seconds\n",
        "      if retry_after.isdigit():\n",
        "        time.sleep(int(retry_after))\n",
        "        # If the value of \"Retry-After\" is a date, parse it and wait until that time\n",
        "      else:\n",
        "        retry_time = datetime.datetime.strptime(retry_after, \"%a, %d %b %Y %H:%M:%S GMT\")\n",
        "        if retry_time > datetime.datetime.now():\n",
        "          time.sleep((retry_time - datetime.datetime.now()).total_seconds())\n",
        "    # Try the request again\n",
        "    response = requests.post('https://query.wikidata.org/sparql?format=json', data=data)\n",
        "  res = response.json()\n",
        "  return res\n",
        "\n",
        "def find_fname_id(name): \n",
        "  find_fn_id_query_male = f'''SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    ?item rdfs:label \"{name}\"@en;\n",
        "          wdt:P31 wd:Q12308941.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "  }}\n",
        "  '''\n",
        "  find_fn_id_query_female = f'''SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    ?item rdfs:label \"{name}\"@en;\n",
        "          wdt:P31 wd:Q12308941.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "  }}\n",
        "  '''\n",
        "  data = {'query': find_fn_id_query_male}\n",
        "  res = retry(data)\n",
        "  if res['results']['bindings'] == []: \n",
        "    data = {'query': find_fn_id_query_female}\n",
        "    res = retry(data)\n",
        "  try:\n",
        "    id = res['results']['bindings'][0]['item']['value'].split('/')[-1]\n",
        "  except IndexError:\n",
        "    #print(f\"Cannot find first name id for {name}\")\n",
        "    id = []\n",
        "  return id\n",
        "\n",
        "def find_lname_id(name): \n",
        "  find_fn_id_query = f'''SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    ?item rdfs:label \"{name}\"@en;\n",
        "          wdt:P31 wd:Q101352.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "  }}\n",
        "  '''\n",
        "  data = {'query': find_fn_id_query}\n",
        "  res = retry(data)\n",
        "  try:\n",
        "      id = res['results']['bindings'][0]['item']['value'].split('/')[-1]\n",
        "  except IndexError:\n",
        "      #print(f\"Cannot find last name id for {name}\")  \n",
        "      id = []  \n",
        "  return id\n",
        "\n",
        "def find_similar_from_id(id): \n",
        "  #find similar names from the ID just found for the target name\n",
        "  id = id\n",
        "  find_similar_query = f\"\"\"\n",
        "  SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    {{\n",
        "      SELECT DISTINCT ?item WHERE {{\n",
        "        ?item p:P460 ?statement0.\n",
        "        ?statement0 (ps:P460/(wdt:P279*)) wd:{id}.\n",
        "      }}\n",
        "    }}\n",
        "  }}\n",
        "  \"\"\"\n",
        "  data = {'query': find_similar_query}\n",
        "  res = retry(data)\n",
        "  similar_name_list = get_itemlabel(res)\n",
        "  return similar_name_list\n",
        "\n",
        "\n",
        "def similar_full_names_separate(name): \n",
        "  fname_similar = []\n",
        "  lname_similar = []\n",
        "  f_name, l_name = name.split(' ')\n",
        "\n",
        "  #open the existing first name and last name txt file and convert both to list\n",
        "  with open(exist_fname_path, 'r') as f:\n",
        "    first_names = f.readlines()\n",
        "  exist_fname_list = []\n",
        "  for line in first_names:\n",
        "    exist_fname_list.append(line.strip())\n",
        "  with open(exist_lname_path, 'r') as f:\n",
        "    last_names = f.readlines()\n",
        "  exist_lname_list = []\n",
        "  for line in last_names:\n",
        "    exist_lname_list.append(line.strip())\n",
        "  \n",
        "  #check if target first name and last name is in the existing list, if not, then proceed with search\n",
        "  #if no id was found, then similar list will return empty else do search from id to find similar names\n",
        "  if f_name not in exist_fname_list: \n",
        "    exist_fname_list.append(f_name)\n",
        "    \n",
        "    f_name_id = find_fname_id(f_name)\n",
        "    if f_name_id == []: \n",
        "      fname_similar = []\n",
        "    else: \n",
        "      fname_similar = find_similar_from_id(f_name_id)\n",
        "\n",
        "    with open(exist_fname_path, 'w') as f:\n",
        "      for line in exist_fname_list:\n",
        "        f.write(line+'\\n')\n",
        "    \n",
        "    with open(similar_fname_path, 'r+') as f:\n",
        "            try:\n",
        "                #load existing data from the file\n",
        "                existing_data = json.load(f)\n",
        "            except:\n",
        "                #if file is empty set existing data to an empty dictionary\n",
        "                existing_data = {}\n",
        "\n",
        "            #add new key-value pair to the dictionary (will be in the same dict)\n",
        "            existing_data[f_name] = fname_similar\n",
        "\n",
        "            #write the updated dictionary back to the file\n",
        "            f.seek(0) #move the file pointer to the beginning of the file (need to double check this)\n",
        "            json.dump(existing_data, f, ensure_ascii=False)\n",
        "            f.truncate() #truncate any remaining content in the file to avoid duplicates\n",
        "\n",
        "  if l_name not in exist_lname_list: \n",
        "    exist_lname_list.append(l_name)\n",
        "    l_name_id = find_lname_id(l_name)\n",
        "    if l_name_id == []: \n",
        "      lname_similar = []\n",
        "    else: \n",
        "      lname_similar = find_similar_from_id(l_name_id)\n",
        "\n",
        "    with open(exist_lname_path, 'w') as f:\n",
        "      for line in exist_lname_list:\n",
        "        f.write(line+'\\n')\n",
        "    \n",
        "    with open(similar_lname_path, 'r+') as f:\n",
        "            try:\n",
        "                #load existing data from the file\n",
        "                existing_data = json.load(f)\n",
        "            except:\n",
        "                #if file is empty set existing data to an empty dictionary\n",
        "                existing_data = {}\n",
        "\n",
        "            #add new key-value pair to the dictionary (will be in the same dict)\n",
        "            existing_data[l_name] = lname_similar\n",
        "\n",
        "            #write the updated dictionary back to the file\n",
        "            f.seek(0) #move the file pointer to the beginning of the file (need to double check this)\n",
        "            json.dump(existing_data, f, ensure_ascii=False)\n",
        "            f.truncate() #truncate any remaining content in the file to avoid duplicates\n",
        "\n",
        "def Merge(dict1, dict2):\n",
        "  return(dict1.update(dict2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz4DKp_fYkM6",
        "outputId": "b6d7c124-8a4d-43ab-a650-84a6b43d924e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='/content/drive/MyDrive/CRP/Datasets/JSON/last_names.json' mode='w' encoding='UTF-8'>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#create new files for existing name records and json file for saving similar names (4 files)\n",
        "open(exist_fname_path, 'w') #if we already ran this first name before, it will be saved to this file\n",
        "open(exist_lname_path, 'w')\n",
        "open(similar_fname_path, 'w') #this is the file for similar first names. one dictionary with multiple keys (target first name)\n",
        "open(similar_lname_path, 'w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFt7SyPcYkM7"
      },
      "outputs": [],
      "source": [
        "#clean all the txt and json files DO THIS WITH CARE\n",
        "\n",
        "with open(exist_fname_path,'r+') as file:\n",
        "    file.truncate(0)\n",
        "with open(exist_lname_path,'r+') as file:\n",
        "    file.truncate(0)\n",
        "with open(similar_fname_path,'r+') as file:\n",
        "    file.truncate(0)\n",
        "with open(similar_lname_path,'r+') as file:\n",
        "    file.truncate(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VATV1Xu3YkM7"
      },
      "outputs": [],
      "source": [
        "#check json and txt files to see the names are added \n",
        "#run joshua baker first then run joshua baker again. you will see nothing is added to the json and txt files\n",
        "#then, run joshua baker and john watson together, you will see john watson is added to the json and txt files\n",
        "similar_full_names_separate('Joshua Baker')\n",
        "similar_full_names_separate('John Watson')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsvaQ5x0N0Zf"
      },
      "source": [
        "# Running the Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oToAbSYFWcc-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = list(names_chunk['Name'])"
      ],
      "metadata": {
        "id": "jZi5eSsfblX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSNiHEuaP5w6",
        "outputId": "1b880770-f0fe-4103-dd7a-80842c1347a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [05:10<00:00,  3.11s/it]\n"
          ]
        }
      ],
      "source": [
        "for name in tqdm(names):\n",
        "    try:\n",
        "        similar_full_names_separate(name)\n",
        "    except:\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}