{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alinehbg/CRP/blob/Anmol_Trial/Extract%20Names/Extract_Wiki_Data_names_try_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "PGVdC2zoVCeE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORsGLFfNJLAE"
      },
      "outputs": [],
      "source": [
        "dir = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "XrNM5SIzJLAE",
        "outputId": "e66bacfb-717c-4660-e80a-d08699f922c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/joshuafan/Documents/GitHub/CRP/Extract Names'"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5qYvvhyqjK9"
      },
      "outputs": [],
      "source": [
        "# names = pd.read_excel('/content/drive/MyDrive/CRP/Datasets/Dataset.xlsx')\n",
        "# names['Name'] = names.apply (lambda x: x['Name'].replace(\".\",\"\") , axis = 1)\n",
        "# names['Name_Length'] = names.apply(lambda x: len(x['Name'].split(\" \")), axis = 1)\n",
        "# names['First_Name_Length'] = names.apply(lambda x: len(x['Name'].split(\" \")[0]), axis = 1)\n",
        "# names = names.loc[names['Name_Length'] == 2]\n",
        "# names = names.loc[names['First_Name_Length'] > 1]\n",
        "# names['Name'] = names.apply(lambda x: x['Name'].title(), axis = 1 )\n",
        "# names = names[[\"Name\"]]\n",
        "# names_list = list(names['Name'])\n",
        "# names_list = list(set(names_list))\n",
        "# names.to_csv('/content/drive/MyDrive/CRP/Datasets/Names_list.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "6d5mnOB_N2wt"
      },
      "outputs": [],
      "source": [
        "names_chunk = pd.read_csv('/Users/joshuafan/Documents/GitHub/CRP/Names_List.csv', chunksize = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67upHfNtN9mQ"
      },
      "source": [
        "# functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8_wJ0gTQVwW0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "QROZ9l8GKICt"
      },
      "outputs": [],
      "source": [
        "def get_itemlabel(query): \n",
        "  name_list = []\n",
        "  for x in query[\"results\"][\"bindings\"]: \n",
        "    name_list.append(x[\"itemLabel\"][\"value\"])\n",
        "  return name_list\n",
        "\n",
        "def retry(data): \n",
        "  response = requests.post('https://query.wikidata.org/sparql?format=json', data=data)\n",
        "  while response.status_code == 429:\n",
        "    retry_after = response.headers.get(\"Retry-After\")\n",
        "    if retry_after:\n",
        "      # If the value of \"Retry-After\" is a number, wait that many seconds\n",
        "      if retry_after.isdigit():\n",
        "        time.sleep(int(retry_after))\n",
        "        # If the value of \"Retry-After\" is a date, parse it and wait until that time\n",
        "      else:\n",
        "        retry_time = datetime.datetime.strptime(retry_after, \"%a, %d %b %Y %H:%M:%S GMT\")\n",
        "        if retry_time > datetime.datetime.now():\n",
        "          time.sleep((retry_time - datetime.datetime.now()).total_seconds())\n",
        "    # Try the request again\n",
        "    response = requests.post('https://query.wikidata.org/sparql?format=json', data=data)\n",
        "  res = response.json()\n",
        "  return res\n",
        "\n",
        "def find_fname_id(name): \n",
        "  find_fn_id_query_male = f'''SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    ?item rdfs:label \"{name}\"@en;\n",
        "          wdt:P31 wd:Q12308941.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "  }}\n",
        "  '''\n",
        "  find_fn_id_query_female = f'''SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    ?item rdfs:label \"{name}\"@en;\n",
        "          wdt:P31 wd:Q12308941.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "  }}\n",
        "  '''\n",
        "  data = {'query': find_fn_id_query_male}\n",
        "  res = retry(data)\n",
        "  if res['results']['bindings'] == []: \n",
        "    data = {'query': find_fn_id_query_female}\n",
        "    res = retry(data)\n",
        "  try:\n",
        "    id = res['results']['bindings'][0]['item']['value'].split('/')[-1]\n",
        "  except IndexError:\n",
        "    #print(f\"Cannot find first name id for {name}\")\n",
        "    id = []\n",
        "  return id\n",
        "\n",
        "def find_lname_id(name): \n",
        "  find_fn_id_query = f'''SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    ?item rdfs:label \"{name}\"@en;\n",
        "          wdt:P31 wd:Q101352.\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "  }}\n",
        "  '''\n",
        "  data = {'query': find_fn_id_query}\n",
        "  res = retry(data)\n",
        "  try:\n",
        "      id = res['results']['bindings'][0]['item']['value'].split('/')[-1]\n",
        "  except IndexError:\n",
        "      #print(f\"Cannot find last name id for {name}\")  \n",
        "      id = []  \n",
        "  return id\n",
        "\n",
        "def find_similar_from_id(id): \n",
        "  #find similar names from the ID just found for the target name\n",
        "  id = id\n",
        "  find_similar_query = f\"\"\"\n",
        "  SELECT DISTINCT ?item ?itemLabel WHERE \n",
        "  {{\n",
        "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    {{\n",
        "      SELECT DISTINCT ?item WHERE {{\n",
        "        ?item p:P460 ?statement0.\n",
        "        ?statement0 (ps:P460/(wdt:P279*)) wd:{id}.\n",
        "      }}\n",
        "    }}\n",
        "  }}\n",
        "  \"\"\"\n",
        "  data = {'query': find_similar_query}\n",
        "  res = retry(data)\n",
        "  similar_name_list = get_itemlabel(res)\n",
        "  return similar_name_list\n",
        "\n",
        "exist_fname_path = '../Data files/first_names.txt'\n",
        "exist_lname_path = '../Data files/last_names.txt'\n",
        "similar_fname_path = '../Data files/first_names.json'\n",
        "similar_lname_path = '../Data files/last_names.json'\n",
        "\n",
        "def similar_full_names_separate(name): \n",
        "  fname_similar = []\n",
        "  lname_similar = []\n",
        "  f_name, l_name = name.split(' ')\n",
        "\n",
        "  #open the existing first name and last name txt file and convert both to list\n",
        "  with open(exist_fname_path, 'r') as f:\n",
        "    first_names = f.readlines()\n",
        "  exist_fname_list = []\n",
        "  for line in first_names:\n",
        "    exist_fname_list.append(line.strip())\n",
        "  with open(exist_lname_path, 'r') as f:\n",
        "    last_names = f.readlines()\n",
        "  exist_lname_list = []\n",
        "  for line in last_names:\n",
        "    exist_lname_list.append(line.strip())\n",
        "  \n",
        "  #check if target first name and last name is in the existing list, if not, then proceed with search\n",
        "  #if no id was found, then similar list will return empty else do search from id to find similar names\n",
        "  if f_name not in exist_fname_list: \n",
        "    exist_fname_list.append(f_name)\n",
        "    \n",
        "    f_name_id = find_fname_id(f_name)\n",
        "    if f_name_id == []: \n",
        "      fname_similar = []\n",
        "    else: \n",
        "      fname_similar = find_similar_from_id(f_name_id)\n",
        "\n",
        "    with open(exist_fname_path, 'w') as f:\n",
        "      for line in exist_fname_list:\n",
        "        f.write(line+'\\n')\n",
        "    \n",
        "    with open(similar_fname_path, 'r+') as f:\n",
        "            try:\n",
        "                #load existing data from the file\n",
        "                existing_data = json.load(f)\n",
        "            except:\n",
        "                #if file is empty set existing data to an empty dictionary\n",
        "                existing_data = {}\n",
        "\n",
        "            #add new key-value pair to the dictionary (will be in the same dict)\n",
        "            existing_data[f_name] = fname_similar\n",
        "\n",
        "            #write the updated dictionary back to the file\n",
        "            f.seek(0) #move the file pointer to the beginning of the file (need to double check this)\n",
        "            json.dump(existing_data, f, ensure_ascii=False)\n",
        "            f.truncate() #truncate any remaining content in the file to avoid duplicates\n",
        "\n",
        "  if l_name not in exist_lname_list: \n",
        "    exist_lname_list.append(l_name)\n",
        "    l_name_id = find_lname_id(l_name)\n",
        "    if l_name_id == []: \n",
        "      lname_similar = []\n",
        "    else: \n",
        "      lname_similar = find_similar_from_id(l_name_id)\n",
        "\n",
        "    with open(exist_lname_path, 'w') as f:\n",
        "      for line in exist_lname_list:\n",
        "        f.write(line+'\\n')\n",
        "    \n",
        "    with open(similar_lname_path, 'r+') as f:\n",
        "            try:\n",
        "                #load existing data from the file\n",
        "                existing_data = json.load(f)\n",
        "            except:\n",
        "                #if file is empty set existing data to an empty dictionary\n",
        "                existing_data = {}\n",
        "\n",
        "            #add new key-value pair to the dictionary (will be in the same dict)\n",
        "            existing_data[l_name] = lname_similar\n",
        "\n",
        "            #write the updated dictionary back to the file\n",
        "            f.seek(0) #move the file pointer to the beginning of the file (need to double check this)\n",
        "            json.dump(existing_data, f, ensure_ascii=False)\n",
        "            f.truncate() #truncate any remaining content in the file to avoid duplicates\n",
        "\n",
        "def Merge(dict1, dict2):\n",
        "  return(dict1.update(dict2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='../Data files/last_names.json' mode='w' encoding='UTF-8'>"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#create new files for existing name records and json file for saving similar names (4 files)\n",
        "open(exist_fname_path, 'w') #if we already ran this first name before, it will be saved to this file\n",
        "open(exist_lname_path, 'w')\n",
        "open(similar_fname_path, 'w') #this is the file for similar first names. one dictionary with multiple keys (target first name)\n",
        "open(similar_lname_path, 'w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "#clean all the txt and json files DO THIS WITH CARE\n",
        "\n",
        "with open(exist_fname_path,'r+') as file:\n",
        "    file.truncate(0)\n",
        "with open(exist_lname_path,'r+') as file:\n",
        "    file.truncate(0)\n",
        "with open(similar_fname_path,'r+') as file:\n",
        "    file.truncate(0)\n",
        "with open(similar_lname_path,'r+') as file:\n",
        "    file.truncate(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check json and txt files to see the names are added \n",
        "#run joshua baker first then run joshua baker again. you will see nothing is added to the json and txt files\n",
        "#then, run joshua baker and john watson together, you will see john watson is added to the json and txt files\n",
        "similar_full_names_separate('Joshua Baker')\n",
        "similar_full_names_separate('John Watson')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsvaQ5x0N0Zf"
      },
      "source": [
        "# requests and the rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oToAbSYFWcc-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LO0rFTIAPN2A"
      },
      "outputs": [],
      "source": [
        "FINAL_DF = pd.DataFrame(columns=['Target_Name', 'Alternate_Name', 'Match'])\n",
        "i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "JSNiHEuaP5w6",
        "outputId": "6e304b57-65aa-4ab7-ae55-00564d9c3f7a"
      },
      "outputs": [],
      "source": [
        "for names in tqdm(names_chunk):\n",
        "  name_list = names['Name'].tolist()\n",
        "  name_dict = {}\n",
        "  for name in name_list: \n",
        "    # temp = similar_full_names(name) (need to change this, function similar_full_names_separate now does not return but add to files directly)\n",
        "    Merge(name_dict, temp)\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  Left = []\n",
        "  Right = []\n",
        "  Match = []\n",
        "\n",
        "  Names = list(name_dict.keys())\n",
        "\n",
        "  for name in Names:\n",
        "    temp = Names\n",
        "    non_names = temp.remove(name)\n",
        "\n",
        "    for alternate in name_dict.get(name):\n",
        "      Left.append(name)\n",
        "      Right.append(alternate)\n",
        "      Match.append(1)\n",
        "    \n",
        "    for non_name in temp:\n",
        "      Left.append(name)\n",
        "      Right.append(non_name)\n",
        "      Match.append(0)\n",
        "      for non_alternate in name_dict.get(non_name):\n",
        "        Left.append(name)\n",
        "        Right.append(non_alternate)\n",
        "        Match.append(0)\n",
        "\n",
        "  #To check\n",
        "  assert len(Left) == len(Right)\n",
        "  assert len(Left) == len(Match)\n",
        "\n",
        "  df['Target_Name'] = Left\n",
        "  df['Alternate_Name'] = Right\n",
        "  df['Match'] = Match\n",
        "  df.to_csv(dir + '/Batches/PART_' + str(i) + \".csv\")\n",
        "#   FINAL_DF = FINAL_DF.append(df)\n",
        "  i += 1\n",
        "#   if i == 10:\n",
        "#     break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eiWjxZxQxDf"
      },
      "outputs": [],
      "source": [
        "FINAL_DF.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQNrsN3PQ6x2"
      },
      "outputs": [],
      "source": [
        "FINAL_DF.to_csv(\"/content/drive/MyDrive/CRP/Datasets/FINAL_DF.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VwYBbEQJLAK",
        "outputId": "a459e84c-47ee-44ad-fe75-940ff71eaacd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZJ8r4whJLAK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
